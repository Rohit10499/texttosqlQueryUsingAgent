{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBUPcILgoF2VAU6TJHaBpL2NXzqBhoTOjk\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# template=\"\"\"Based on table schema below, write a SQL query that would answer the user's question:\n",
    "# {schema}\n",
    "# Question: {question}\n",
    "# SQL Query\n",
    "# \"\"\"\n",
    "\n",
    "# prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Based on table schema below, write a SQL query that would answer the user's question:\\nmyschema\\nQuestion: how are you?\\nSQL Query\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt.format(schema=\"myschema\",question=\"how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=SQLDatabase.from_uri(\"sqlite:///mydatabase.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "import requests\n",
    "\n",
    "class GeminiLLM(LLM):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def _call(self, prompt: str, stop=None):\n",
    "        # Replace with the actual Gemini API call\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 1000,  # Adjust as needed\n",
    "            # Add any necessary parameters required by Gemini API\n",
    "        }\n",
    "\n",
    "        response = requests.post('https://api.google.com/gemini/v1/generate', headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['text']  # Assuming response has a \"text\" field for completion\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"gemini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Result: SELECT COUNT(*) FROM artists;\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Passthrough class to bypass RunnablePassthrough issues\n",
    "class RunnablePassthrough:\n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "\n",
    "    def assign(self, schema):\n",
    "        self.schema = schema\n",
    "        return self\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        inputs[\"schema\"] = self.schema\n",
    "        return inputs\n",
    "\n",
    "# Dummy GeminiLLM class (Replace with your actual LLM)\n",
    "class GeminiLLM:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def bind(self, stop):\n",
    "        return self\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return \"SQL Result: SELECT COUNT(*) FROM artists;\"  # Mock output for testing\n",
    "\n",
    "# Assume get_schema is a function that provides the schema information\n",
    "def get_schema():\n",
    "    return \"CREATE TABLE artists (id INTEGER PRIMARY KEY, name TEXT);\"\n",
    "\n",
    "# Get schema\n",
    "schema = get_schema()\n",
    "\n",
    "# Define your prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Based on table schema below, write a SQL query that would answer the user's question:\n",
    "    {schema}\n",
    "    Answer the following question in SQL format:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM (replace with the actual model)\n",
    "gemini_llm = GeminiLLM(api_key='AIzaSyBUPcILgoF2VAU6TJHaBpL2NXzqBhoTOjk')\n",
    "\n",
    "# Create the LLM chain with the manual passthrough class\n",
    "sql_chain = (\n",
    "    RunnablePassthrough(schema=schema)  # Manually pass the schema\n",
    "    | prompt_template\n",
    "    | gemini_llm.bind(stop=\"\\nSQL Result :\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "question = \"how many artists are there?\"\n",
    "result = sql_chain.invoke({\"question\": question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
